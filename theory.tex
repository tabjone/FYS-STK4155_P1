%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Depending on what type of methods we use, may not need to have subsubsection.

\section{Theory}
\subsection{Linear regression}
In machine learning we call the independent variable $\mathbf{x}$ a feature and the dependent variable $\mathbf{y}$ a response. A regression model aims at finding a likelihood function $p(\mathbf{y}|\mathbf{x})$, that is the conditional distribution for $\mathbf{y}$ with a given $\mathbf{x}$. The estimation of $p(\mathbf{y}|\mathbf{x})$ is made using a data set with $n$ cases $i=0,1,2,...,n-1$ of a response variable $y_i$ and a set of predictor variables $\mathbf{x}_i=[x_{i0}, x_{i1},...,x_{ip-1}]$. The set of these vectors $\mathbf{X}=[\mathbf{x}_{0}\ \mathbf{x}_{1}\ ...\ \mathbf{x}_{n-1}]$ is called the design matrix of the model. 


The aim of regression analysis is to explain $\mathbf y$ in terms of $\mathbf X$ through a functional relationship like $y_i=f(\mathbf{X}_{i*})$. When no prior knowledge on the form of $f( \cdot )$ is available, it is common to assume a linear relationship between $\mathbf{X}$ and $\mathbf{y}$. This assumption gives rise to the linear regression model where $\boldsymbol\beta=\left[\beta_0, \beta_1, ..., \beta_{p-1} \right]^T$ are the regression parameters and the error variable $\boldsymbol\epsilon$ is an unobserved random variable that adds "noise" to the linear relationship between the dependent variable and regressors, so that our model becomes

$$
\tilde{y}(x_i) = \sum_{j=0}^{p-1} \beta_j x_{ij}=\mathbf X_{i*}\boldsymbol{\beta}.
$$

Giving that the dependent variable is

$$
y(x_i) = \tilde{y}_i + \epsilon_i = \mathbf X_{i*}\boldsymbol{\beta} + \epsilon_i.
$$

These $n$ equations can be written on matrix form as
\begin{equation}\label{eq:linear_regression}
    \mathbf{y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}.
\end{equation}

When assuming that the noise $\epsilon$ is normally distributed around zero with a standard deviation of $\sigma$, we can calculate the expectation value

\begin{equation}\label{eq:expectation_y}
\mathbb{E}[\mathbf y] = \mathbf{X}\boldsymbol{\beta}
\end{equation}
and the variance

\begin{equation}\label{eq:variance_y}
\mbox{Var}[ y_i] = \sigma^2.
\end{equation}
The calculations of these values can be found in appendix \ref{app:ols_expactation_variance}

\subsubsection{Ordinary least squares}

The method of ordinary least squares computes the unique line that minimises the sum of squared differences between the true data and that line. In other words we have an optimisation problem on the from
$$
{\displaystyle \min_{\boldsymbol{\beta}\in
{\mathbb{R}}^{p}}}\frac{1}{n}\sum_{i=0}^{n-1}\left(y_i-\tilde{y}_i\right)^2.
$$

We define the cost function for ordinary least squares as

$$
C(\boldsymbol\beta) = \frac{1}{n}\sum_{i=0}^{n-1}(y_i-\tilde{y}_i)^2=\frac{1}{n}\left[(\mathbf{y}-\mathbf{\tilde y})^T(\mathbf{y}-\mathbf{\tilde y}) \right].
$$


Inserting the predicted values $\mathbf{\tilde y}=\mathbf{X}\boldsymbol\beta$ we get

$$
C(\boldsymbol\beta) = \frac{1}{n}\sum_{i=0}^{n-1}(y_i-\tilde{y}_i)^2=\frac{1}{n}\left[(\mathbf{y}-\mathbf{X}\boldsymbol\beta)^T(\mathbf{y}-\mathbf{X}\boldsymbol\beta) \right].
$$

We call optimal parameter $\hat{\boldsymbol\beta}$ the one that minimises the cost function. This will be a zero of the derivative

$$
\frac{\partial \boldsymbol\beta}{\partial \beta_j} = 0.
$$

Doing this derivative gives

$$
\mathbf{X}^T (\mathbf{y}-\mathbf{X}\hat{\boldsymbol{\beta}} )=0.
$$

Rewriting this we get that

\begin{equation}\label{eq:beta_OLS}
    \hat{\boldsymbol\beta}_{OLS}=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}
\end{equation}
is the optimal parameter if the Hessian matrix $\mathbf{H}=(\mathbf{X}^T\mathbf{X})$ is invertible. This has expectation value 
\begin{equation}\label{eq:expectation_beta}
\mathbb{E}(\boldsymbol{\hat{\beta}}) =\boldsymbol{\beta}
\end{equation}
and variance
\begin{equation}\label{eq:variance_beta}
\mbox{Var}(\boldsymbol{\hat{\beta}})  = \sigma^2 \, (\mathbf{X}^{T} \mathbf{X})^{-1}.
\end{equation}
The calculations for this can be found in appendix \ref{app:ols_expactation_variance}.


\subsubsection{Ridge}
The ordinary least squares method can be inaccurate when the model have highly correlated independent variables. Ridge regression tries to solve this by adding a regularization parameter $\lambda$, called a hyperparameter, to the optimization problem. We start by re-writing the optimization problem as 
$$
{\displaystyle \min_{\boldsymbol{\beta}\in
{\mathbb{R}}^{p}}}\frac{1}{n}\sum_{i=0}^{n-1}\left(y_i-\tilde{y}_i\right)^2=\frac{1}{n}\vert\vert \boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta}\vert\vert_2^2,
$$
where we have used the definition of  a norm-2 vector, that is
$$
\vert\vert \boldsymbol{x}\vert\vert_2 = \sqrt{\sum_i x_i^2}.
$$
By adding the regularization parameter $\lambda$ we get that
$$
{\displaystyle \min_{\boldsymbol{\beta}\in
{\mathbb{R}}^{p}}}\frac{1}{n}\vert\vert \boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta}\vert\vert_2^2+\lambda\vert\vert \boldsymbol{\beta}\vert\vert_2^2,
$$
where we
require that $\vert\vert \boldsymbol{\beta}\vert\vert_2^2\le t$, where $t$ is
a finite number larger than zero. We define the cost function to be optimized, that is
$$
C(\boldsymbol{X},\boldsymbol{\beta})=\left\{(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta})^T(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta})\right\}+\lambda\boldsymbol{\beta}^T\boldsymbol{\beta},
$$
Minimizing the above equation in the same way as we did OLS gives the optimal parameter
\begin{equation}\label{eq:beta_ridge}
\hat{\boldsymbol{\beta}}_{\mathrm{Ridge}} = \left(\boldsymbol{X}^T\boldsymbol{X}+\lambda\boldsymbol{I}\right)^{-1}\boldsymbol{X}^T\boldsymbol{y}.
\end{equation}
This means that the Ridge estimator scales the OLS estimator by the inverse of a factor $(1+\lambda)$.




\subsubsection{Lasso}
For Lasso or least absolute shrinkage and selection operator we define the optimization problem
$$
{\displaystyle \min_{\boldsymbol{\beta}\in
{\mathbb{R}}^{p}}}\frac{1}{n}\vert\vert \boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta}\vert\vert_2^2+\lambda\vert\vert \boldsymbol{\beta}\vert\vert_1,
$$
with the norm-1 as
$$
\vert\vert \boldsymbol{x}\vert\vert_1 = \sum_i \vert x_i\vert.
$$
And this gives us the cost function 
$$
C(\boldsymbol{X},\boldsymbol{\beta})=\left\{(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta})^T(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta})\right\}+\lambda\vert\vert\boldsymbol{\beta}\vert\vert_1.
$$
Taking the derivative with respect to $\boldsymbol{\beta}$ and recalling that the derivative of the absolute value is (we drop the boldfaced vector symbol for simplicty)
$$
\frac{d \vert \beta\vert}{d \boldsymbol{\beta}}=\mathrm{sgn}(\boldsymbol{\beta})=\left\{\begin{array}{cc} 1 & \beta > 0 \\-1 & \beta < 0, \end{array}\right.
$$
we have that the derivative of the cost function is
$$
\frac{\partial C(\boldsymbol{X},\boldsymbol{\beta})}{\partial \boldsymbol{\beta}}=-2\boldsymbol{X}^T(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta})+\lambda sgn(\boldsymbol{\beta})=0,
$$
and reordering we have
$$
\boldsymbol{X}^T\boldsymbol{X}\boldsymbol{\beta}+\lambda sgn(\boldsymbol{\beta})=2\boldsymbol{X}^T\boldsymbol{y}.
$$
This equation does not lead to a nice analytical equation as in either Ridge regression or ordinary least squares. This equation can however be solved by using standard convex optimization algorithms using for example the Python package CV\slash XOPT.




\subsection{Franke Function NOT DONE}
THIS SHOULD MAYBE BE IN THE METHOD SECTION. Since this is more a way to generate a dataset than a theoretical function?

\subsection{Resampling methods NOT DONE}
men dette har litt med bias varias tradeoff Ã¥ gjÃ¸re?
\subsubsection{Bootstrap}
\subsubsection{Cross-validation}

\subsection{Bias-Variance tradeoff NOT DONE}
I HAVE NOT STARTED WRITING HERE YET! JUST PLUGGED IN EQUATIONS!

Assumption for this calculation: y=f(x)+epsilon. And f(x) is assumed to be a deterministic function. This makes $E[f]=f=E[y]$ (unbiased function)

$$
\mathbb{E}\left[(\mathbf{y}-\mathbf{\tilde{y}})^2\right]=\mathbb{E}\left[(\mathbf{f}+\mathbf{\epsilon}-\mathbf{\tilde{y}})^2\right],
$$

Adding and subtracting $\mathbb{E}\left[\mathbf{\tilde{y}}\right]$

$$
\mathbb{E}\left[(\mathbf{y}-\mathbf{\tilde{y}})^2\right]=\mathbb{E}\left[(\mathbf{f}+\mathbf{\epsilon}-\mathbf{\tilde{y}}+\mathbb{E}\left[\mathbf{\tilde{y}}\right]-\mathbb{E}\left[\mathbf{\tilde{y}}\right])^2\right],
$$

$$
\mathbb{E}\left[(\mathbf{y}-\mathbf{\tilde{y}})^2\right]=\mathbb{E}\left[(\mathbf{y}-\mathbb{E}\left[\mathbf{\tilde{y}}\right])^2\right]+\mathrm{Var}\left[\mathbf{\tilde{y}}\right]+\sigma^2,
$$

$$
\mathbb{E}\left[(\mathbf{y}-\mathbf{\tilde{y}})^2\right]=\left(\mbox{Bias}\left[{\tilde y}\right] \right)^2 + \mbox{Var}\left[{\tilde f} \right] + \sigma^2
$$

