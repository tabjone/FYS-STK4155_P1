%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Depending on what type of methods we use, may not need to have subsubsection.

\section{Theory}
\subsection{Linear regression}
In machine learning we call the independent variable $\mathbf{x}$ a feature and the dependent variable $\mathbf{y}$ a response. A regression model aims at finding a likelihood function $p(\mathbf{y}|\mathbf{x})$, that is the conditional distribution for $\mathbf{y}$ with a given $\mathbf{x}$. The estimation of $p(\mathbf{y}|\mathbf{x})$ is made using a data set with $n$ cases $i=0,1,2,...,n-1$ of a response variable $y_i$ and a set of predictor variables $\mathbf{x}_i=[x_{i0}, x_{i1},...,x_{ip-1}]$. The set of these vectors $\mathbf{X}=[\mathbf{x}_{0}\ \mathbf{x}_{1}\ ...\ \mathbf{x}_{n-1}]$ is called the design matrix of the model. 


The aim of regression analysis is to explain $\mathbf y$ in terms of $\mathbf X$ through a functional relationship like $y_i=f(\mathbf{X}_{i*})$. When no prior knowledge on the form of $f( \cdot )$ is available, it is common to assume a linear relationship between $\mathbf{X}$ and $\mathbf{y}$. This assumption gives rise to the linear regression model where $\boldsymbol\beta=\left[\beta_0, \beta_1, ..., \beta_{p-1} \right]^T$ are the regression parameters and the error variable $\boldsymbol\epsilon$ is an unobserved random variable that adds "noise" to the linear relationship between the dependent variable and regressors, so that our model becomes

$$
\tilde{y}(x_i) = \sum_{j=0}^{p-1} \beta_j x_{ij}=\mathbf X_{i*}\boldsymbol{\beta}.
$$

Giving that the dependent variable is

$$
y(x_i) = \tilde{y}_i + \varepsilon_i = \mathbf X_{i*}\boldsymbol{\beta} + \epsilon_i.
$$

These $n$ equations can be written on matrix form as
\begin{equation}\label{eq:linear_regression}
    \mathbf{y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}.
\end{equation}

We get that the expectations value of this is

\begin{equation}\label{eq:expectation_y}
\mathbb{E}[\mathbf y] = \mathbf{X}\boldsymbol{\beta}
\end{equation}
and that the variance is

OBS OBS!!!! HER SLUTTET DU PÅ TOG!!!!OBS OBS!!!! HER SLUTTET DU PÅ TOG!!!!
\begin{equation}\label{eq:variance_y}
\mbox{Var}
\end{equation}
OBS OBS!!!! HER SLUTTET DU PÅ TOG!!!!OBS OBS!!!! HER SLUTTET DU PÅ TOG!!!!

\subsubsection{Ordinary least squares}

The method of ordinary least squares computes the unique line that minimises the sum of squared differences between the true data and that line. 

We define the cost function for ordinary least squares as the mean squared error

$$
C(\boldsymbol\beta) = \frac{1}{n}\sum_{i=0}^{n-1}(y_i-\tilde{y}_i)^2=\frac{1}{n}\left[(\mathbf{y}-\mathbf{\tilde y})^T(\mathbf{y}-\mathbf{\tilde y}) \right].
$$


Inserting the predicted values $\mathbf{\tilde y}=\mathbf{X}\boldsymbol\beta$ we get

$$
C(\boldsymbol\beta) = \frac{1}{n}\sum_{i=0}^{n-1}(y_i-\tilde{y}_i)^2=\frac{1}{n}\left[(\mathbf{y}-\mathbf{X}\boldsymbol\beta)^T(\mathbf{y}-\mathbf{X}\boldsymbol\beta) \right].
$$

We call optimal parameter $\hat{\boldsymbol\beta}$ the one that minimises the cost function. This will be a zero of the derivative

$$
\frac{\partial \boldsymbol\beta}{\partial \beta_j} = 0.
$$

Doing this derivative we get that

$$
\mathbf{X}^T (\mathbf{y}-\mathbf{X}\hat{\boldsymbol{\beta}} )=0.
$$

Rewriting this we get that

\begin{equation}\label{eq:beta_OLS}
    \hat{\boldsymbol\beta}=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}
\end{equation}
is the optimal parameter if the Hessian matrix $\mathbf{H}=(\mathbf{X}^T\mathbf{X})$ is invertible. 


In appendix \ref{app:ols_expactation_variance}.

OG HER REFERER DU TIL APPENDIX OSV. STATE EXPECTATION VALUE OG VARIANCE FOR DE TO GREIENE!!!!
!!!!
!!!
\subsubsection{Ridge}

\subsubsection{Lasso}

\subsection{Bias-Variance tradeoff}
I HAVE NOT STARTED WRITING HERE YET! JUST PLUGGED IN EQUATIONS!

Assumption for this calculation: y=f(x)+epsilon. And f(x) is assumed to be a deterministic function. This makes $E[f]=f=E[y]$ (unbiased function)

$$
\mathbb{E}\left[(\mathbf{y}-\mathbf{\tilde{y}})^2\right]=\mathbb{E}\left[(\mathbf{f}+\mathbf{\epsilon}-\mathbf{\tilde{y}})^2\right],
$$

Adding and subtracting $\mathbb{E}\left[\mathbf{\tilde{y}}\right]$

$$
\mathbb{E}\left[(\mathbf{y}-\mathbf{\tilde{y}})^2\right]=\mathbb{E}\left[(\mathbf{f}+\mathbf{\epsilon}-\mathbf{\tilde{y}}+\mathbb{E}\left[\mathbf{\tilde{y}}\right]-\mathbb{E}\left[\mathbf{\tilde{y}}\right])^2\right],
$$

$$
\mathbb{E}\left[(\mathbf{y}-\mathbf{\tilde{y}})^2\right]=\mathbb{E}\left[(\mathbf{y}-\mathbb{E}\left[\mathbf{\tilde{y}}\right])^2\right]+\mathrm{Var}\left[\mathbf{\tilde{y}}\right]+\sigma^2,
$$

$$
\mathbb{E}\left[(\mathbf{y}-\mathbf{\tilde{y}})^2\right]=\left(\mbox{Bias}\left[{\tilde y}\right] \right)^2 + \mbox{Var}\left[{\tilde f} \right] + \sigma^2
$$

