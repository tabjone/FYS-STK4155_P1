%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Theory}
\subsection{Linear regression}
%Half and half copied / self written
In machine learning we call the independent variable $\mathbf{x}$ a feature and the dependent variable $\mathbf{y}$ a response. A regression model aims at finding a likelihood function $p(\mathbf{y}|\mathbf{x})$, that is the conditional distribution for $\mathbf{y}$ with a given $\mathbf{x}$. The estimation of $p(\mathbf{y}|\mathbf{x})$ is made using a data set with $n$ cases $i=0,1,2,...,n-1$ of a response variable $y_i$ and a set of predictor variables $\mathbf{x}_i=[x_{i0}, x_{i1},...,x_{ip-1}]$. The set of these vectors $\mathbf{X}=[\mathbf{x}_{0}\ \mathbf{x}_{1}\ ...\ \mathbf{x}_{n-1}]$ is called the design matrix of the model. 


The aim of regression analysis is to explain $\mathbf y$ in terms of $\mathbf X$ through a functional relationship like $y_i=f(\mathbf{X}_{i*})$. When no prior knowledge on the form of $f( \cdot )$ is available, it is common to assume a linear relationship between $\mathbf{X}$ and $\mathbf{y}$. This assumption gives rise to the linear regression model where $\boldsymbol\beta=\left[\beta_0, \beta_1, ..., \beta_{p-1} \right]^T$ are the regression parameters and the error variable $\boldsymbol\epsilon$ is an unobserved random variable that adds "noise" to the linear relationship between the dependent variable and regressors, so that our model becomes

$$
\tilde{y}(x_i) = \sum_{j=0}^{p-1} \beta_j x_{ij}=\mathbf X_{i*}\boldsymbol{\beta}.
$$

Giving that the dependent variable is

$$
y(x_i) = \tilde{y}_i + \epsilon_i = \mathbf X_{i*}\boldsymbol{\beta} + \epsilon_i.
$$

These $n$ equations can be written on matrix form as
\begin{equation}\label{eq:linear_regression}
    \mathbf{y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}.
\end{equation}

When assuming that the noise $\epsilon$ is normally distributed around zero with a standard deviation of $\sigma$, we can calculate the expectation value

\begin{equation}\label{eq:expectation_y}
\mathbb{E}[\mathbf y] = \mathbf{X}\boldsymbol{\beta}
\end{equation}
and the variance

\begin{equation}\label{eq:variance_y}
\mbox{Var}[ y_i] = \sigma^2.
\end{equation}
The calculations of these values can be found in appendix \ref{app:ols_expactation_variance}

\subsubsection{Ordinary least squares}
%%FIrst sentence here is from wikipedia
The method of ordinary least squares computes the unique line that minimises the sum of squared differences between the true data and that line. In other words we have an optimisation problem on the from
$$
{\displaystyle \min_{\boldsymbol{\beta}\in
{\mathbb{R}}^{p}}}\frac{1}{n}\sum_{i=0}^{n-1}\left(y_i-\tilde{y}_i\right)^2.
$$

We define the cost function for ordinary least squares as

$$
C(\boldsymbol\beta) = \frac{1}{n}\sum_{i=0}^{n-1}(y_i-\tilde{y}_i)^2=\frac{1}{n}\left[(\mathbf{y}-\mathbf{\tilde y})^T(\mathbf{y}-\mathbf{\tilde y}) \right].
$$


Inserting the predicted values $\mathbf{\tilde y}=\mathbf{X}\boldsymbol\beta$ we get

$$
C(\boldsymbol\beta) = \frac{1}{n}\sum_{i=0}^{n-1}(y_i-\tilde{y}_i)^2=\frac{1}{n}\left[(\mathbf{y}-\mathbf{X}\boldsymbol\beta)^T(\mathbf{y}-\mathbf{X}\boldsymbol\beta) \right].
$$

We call optimal parameter $\hat{\boldsymbol\beta}$ the one that minimises the cost function. This will be a zero of the derivative

$$
\frac{\partial \boldsymbol\beta}{\partial \beta_j} = 0.
$$

Doing this derivative gives

$$
\mathbf{X}^T (\mathbf{y}-\mathbf{X}\hat{\boldsymbol{\beta}} )=0.
$$

Rewriting this we get that

\begin{equation}\label{eq:beta_OLS}
    \hat{\boldsymbol\beta}_{OLS}=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}
\end{equation}
is the optimal parameter if the Hessian matrix $\mathbf{H}=(\mathbf{X}^T\mathbf{X})$ is invertible. This has expectation value 
\begin{equation}\label{eq:expectation_beta}
\mathbb{E}(\boldsymbol{\hat{\beta}}) =\boldsymbol{\beta}
\end{equation}
and variance
\begin{equation}\label{eq:variance_beta}
\mbox{Var}(\boldsymbol{\hat{\beta}})  = \sigma^2 \, (\mathbf{X}^{T} \mathbf{X})^{-1}.
\end{equation}
The calculations for this can be found in appendix \ref{app:ols_expactation_variance}.


\subsubsection{Ridge}
The ordinary least squares method can be inaccurate when the model have highly correlated independent variables. Ridge regression tries to solve this by adding a regularization parameter $\lambda$, called a hyperparameter, to the optimization problem. We start by re-writing the optimization problem as 
%%FROM HERE IS COPIED ALMOST ENIRELY FROM NOTES
$$
{\displaystyle \min_{\boldsymbol{\beta}\in
{\mathbb{R}}^{p}}}\frac{1}{n}\sum_{i=0}^{n-1}\left(y_i-\tilde{y}_i\right)^2=\frac{1}{n}\vert\vert \boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta}\vert\vert_2^2,
$$
where we have used the definition of  a norm-2 vector, that is
$$
\vert\vert \boldsymbol{x}\vert\vert_2 = \sqrt{\sum_i x_i^2}.
$$
By adding the regularization parameter $\lambda$ we get that
$$
{\displaystyle \min_{\boldsymbol{\beta}\in
{\mathbb{R}}^{p}}}\frac{1}{n}\vert\vert \boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta}\vert\vert_2^2+\lambda\vert\vert \boldsymbol{\beta}\vert\vert_2^2,
$$
where we
require that $\vert\vert \boldsymbol{\beta}\vert\vert_2^2\le t$, where $t$ is
a finite number larger than zero. We define the cost function to be optimized, that is
$$
C(\boldsymbol{X},\boldsymbol{\beta})=\left\{(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta})^T(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta})\right\}+\lambda\boldsymbol{\beta}^T\boldsymbol{\beta},
$$
Minimizing the above equation in the same way as we did OLS gives the optimal parameter
\begin{equation}\label{eq:beta_ridge}
\hat{\boldsymbol{\beta}}_{\mathrm{Ridge}} = \left(\boldsymbol{X}^T\boldsymbol{X}+\lambda\boldsymbol{I}\right)^{-1}\boldsymbol{X}^T\boldsymbol{y}.
\end{equation}
This means that the Ridge estimator scales the OLS estimator by the inverse of a factor $(1+\lambda)$.


\subsubsection{Lasso}
%% THIS ENTIRE THING IS COPIED FROM THE LECTURE NOTES
For Lasso or least absolute shrinkage and selection operator we define the optimization problem
$$
{\displaystyle \min_{\boldsymbol{\beta}\in
{\mathbb{R}}^{p}}}\frac{1}{n}\vert\vert \boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta}\vert\vert_2^2+\lambda\vert\vert \boldsymbol{\beta}\vert\vert_1,
$$
with the norm-1 as
$$
\vert\vert \boldsymbol{x}\vert\vert_1 = \sum_i \vert x_i\vert.
$$
And this gives us the cost function 
$$
C(\boldsymbol{X},\boldsymbol{\beta})=\left\{(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta})^T(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta})\right\}+\lambda\vert\vert\boldsymbol{\beta}\vert\vert_1.
$$
Taking the derivative with respect to $\boldsymbol{\beta}$ and recalling that the derivative of the absolute value is (we drop the boldfaced vector symbol for simplicty)
$$
\frac{d \vert \beta\vert}{d \boldsymbol{\beta}}=\mathrm{sgn}(\boldsymbol{\beta})=\left\{\begin{array}{cc} 1 & \beta > 0 \\-1 & \beta < 0, \end{array}\right.
$$
we have that the derivative of the cost function is
$$
\frac{\partial C(\boldsymbol{X},\boldsymbol{\beta})}{\partial \boldsymbol{\beta}}=-2\boldsymbol{X}^T(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta})+\lambda sgn(\boldsymbol{\beta})=0,
$$
and reordering we have
$$
\boldsymbol{X}^T\boldsymbol{X}\boldsymbol{\beta}+\lambda sgn(\boldsymbol{\beta})=2\boldsymbol{X}^T\boldsymbol{y}.
$$
This equation does not lead to a nice analytical equation as in either Ridge regression or ordinary least squares. This equation can however be solved by using standard convex optimization algorithms using for example the Python package CV\slash XOPT.

\subsection{Resampling methods and Bias-Variance Tradeoff}
When working with a dataset of independent and identically distributed events, called IIDs, we can draw repeated samples from the training set and refit the model on each sample. For each model refitting we can obtain additional information that would not be attainable by only fitting the model once. This will be more computationally expensive as we need to fit the same model over and over again. Let's call the number of fits for $m$. Then we use the central limit theorem to get the approximation of the standard deviation
\begin{equation}\label{eq:central_limit_deviation}
\sigma_m\approx
\frac{\sigma}{\sqrt{m}}.
\end{equation}
This equality is true when $m$ goes to infinity and a good approximation for large $m$ values. What this equation says is that the more we resample a dataset, the lower the variance gets. But this also means that if we have erroneous assumptions in our model, these assumptions will be strengthened. In other words we get higher bias, and this can make us miss relevant relations between features and responses. This is called underfitting. On the other hand, high variance can be an indication of overfitting. The bias-variance tradeoff is the problem of trying to minimize the error in both variance and bias at the same time. 

To see problem clearer we will re-write the cost function for the ordinary least squares method and show that this is dependent on the variance and bias of the model. Since this cost function is the mean squared error, we can write it as
$$
C(\boldsymbol\beta) = \mathbb{E}\left[(\mathbf{y}-\mathbf{\tilde{y}})^2\right] = \mathbb{E}\left[(\mathbf{f}+\mathbf{\epsilon}-\mathbf{\tilde{y}})^2\right],
$$

where $f(x)$ is assumed to be deterministic, so that $\mathbb{E}[\mathbf{f}]=\mathbf{f}=\mathbb{E}[\mathbf{y}]$, or in other words $f$ is unbiased. We now take the equation of the cost function
$$
\mathbb{E}\left[(\mathbf{f}+\mathbf{\epsilon}-\mathbf{\tilde{y}})^2\right]=\mathbb{E}\left[(\mathbf{f}+\mathbf{\epsilon}-\mathbf{\tilde{y}})^2\right],
$$
and by adding and subtracting $\mathbb{E}\left[\mathbf{\tilde{y}}\right]$ in the parenthesis on the right side, we get

\begin{align*}
\mathbb{E}\left[(\mathbf{y}-\mathbf{\tilde{y}})^2\right]
&=\mathbb{E}\left[(\mathbf{f}+\mathbf{\epsilon}-\mathbf{\tilde{y}}+\mathbb{E}\left[\mathbf{\tilde{y}}\right]-\mathbb{E}\left[\mathbf{\tilde{y}}\right])^2\right]
\\
&= \mathbb{E}\left[(\mathbf{f}-\mathbb{E}[\mathbf{\tilde y}])^2 \right] + \mathbb{E}\left[(\mathbf{\tilde y}-\mathbb{E}[\mathbf{\tilde{y}}])^2\right] + \mathbb{E}[\epsilon^2] + 2(f-\mathbb{E}[\mathbf{\tilde y}])\mathbb{E}[\epsilon]
\\
&+2\mathbb{E}[\epsilon]\mathbb{E}\left[\mathbb{E}[\mathbf{\tilde y}]-\mathbf{\tilde y} \right] + 2\mathbb{E}\left[\mathbb{E}[\mathbf{\tilde y}] -\mathbf{\tilde y} \right](f - \mathbb{E}[\mathbf{\tilde y}])
\\
&=\mathbb{E}\left[(\mathbf{f}-\mathbb{E}[\mathbf{\tilde y}])^2 \right]  + \mathbb{E}\left[(\mathbf{\tilde y}-\mathbb{E}[\mathbf{\tilde{y}}])^2\right]+ \mathbb{E}[\epsilon^2] + 2\mathbb{E}\left[\mathbb{E}[\mathbf{\tilde y}] -\mathbf{\tilde y} \right](f - \mathbb{E}[\mathbf{\tilde y}])
\\
&= \mathbb{E}\left[(\mathbf{f}-\mathbb{E}[\mathbf{\tilde y}])^2 \right]  +\mathbb{E}\left[(\mathbf{\tilde y}-\mathbb{E}[\mathbf{\tilde{y}}])^2\right] + \mathbb{E}[\epsilon^2]
\\
&=\left(\mbox{Bias}\left[{\tilde y}\right] \right)^2 + \mbox{Var}\left[{\tilde f} \right] + \sigma^2,
\end{align*}

where we used that $\mathbb{E}\left[\mathbb{E}[\mathbf{\tilde y}]\right]=\mathbb{E}[\mathbf{\tilde y}]$, 

\begin{equation}\label{eq:bias_ytilde}
(\mathrm{Bias}[\tilde{y}])^2=\left(\boldsymbol{y}-\mathbb{E}\left[\boldsymbol{\tilde{y}}\right]\right)^2
\end{equation}
and
\begin{equation}\label{eq:variance_ftilde}
\mathrm{var}[\tilde{f}]=\mathbb{E}\left[(\mathbf{\tilde y}-\mathbb{E}[\mathbf{\tilde{y}}])^2\right].
\end{equation}

Since $\sigma^2$ is the variance of the dataset there is not much we can do about this. So what plays a role in the mean squared error is the bias and variance of the model. This means that to reduce the mean squared error we need to minimize these two parameters. 


%From this equation it's obvious that we can't reduce $\sigma^2$ as this is from the noise in the dataset. 
%From task
 %Explain what the terms mean and discuss their interpretations.



%In this chapter, we discuss two of the most commonly used resampling methods, cross-validation and the bootstrap. %Both methods are important tools in the practical application of many statistical learning procedures. 
%For example, cross-validation can be used to estimate the test error associated with a given statistical learning method %in order to evaluate its performance, or to select the appropriate level of flexibility. The process of evaluating a modelâ€™s %performance is known as model assessment, whereas the process of selecting the proper level of flexibility for a model %is known as model selection. The bootstrap is widely used.

\subsubsection{Bootstrap NOT DONE}
The steps of the independent bootstrap is:

\begin{mdframed}[backgroundcolor=black!10]
\raggedright
1. Draw with replacement $n$ numbers for the observed variables $\boldsymbol{x} = (x_1,x_2,\cdots,x_n)$. \\
2. Define a vector $\boldsymbol{x}^*$ containing the values which were drawn from $\boldsymbol{x}$. \\
3. Using the vector $\boldsymbol{x}^*$ compute $\boldsymbol{\hat\beta}^*$ by evaluating $\boldsymbol{\hat\beta}$ under the observations $\boldsymbol{x}^*$. \\
4. Repeat this process $k$ times. 
\end{mdframed}

%very close to lecture notes
The advantage of bootstrapping is that i's very general, as it does not require distributional assumptions. This makes bootstrap more accurate when data is not well behaved or the sample size is small. We use that when
 $\boldsymbol{\hat{\beta}}=\boldsymbol{\hat\beta}(\boldsymbol{X})$ is a function of random variables, $\boldsymbol{\hat\beta}$ itself must be a random variable. Thus it has
a pdf $p(\boldsymbol{x})$. The aim of the bootstrap is to
estimate $p(\boldsymbol{x})$ by the relative frequency of
$\boldsymbol{\hat\beta}$. If the relative frequency closely
resembles $p(\boldsymbol{x})$, then using numerics, it is straight forward to
estimate all the interesting parameters of $p(\boldsymbol{x})$ using point
estimators. $p(x)$. If we draw observations in accordance with
the relative frequency of the observations, we can replace $p(x)$ by the relative frequency of the observation $X_i$. And in the case that $\boldsymbol{\hat\beta}$ has
more than one component, and the components are independent, we use the
same estimator on each component separately. 
The histogram of the relative frequency of $\boldsymbol{\hat\beta}^*$ is then the estimate of the probability distribution $p(x)$. This is however not of big interest as we want to look at the mean squared error, bias and variance which we find by using those estimators on $\boldsymbol{\hat\beta}^*$.

\subsubsection{Cross-validation NOT DONE}
The steps of cross-validation for the various values of $k$ is:
\begin{mdframed}[backgroundcolor=black!10]
\raggedright

1. Shuffle the dataset randomly.\\

2. Split the dataset into $k$ groups.\\

3. For each unique group:\\

\hspace{1cm}a. Decide which group to use as a set for test data\\

\hspace{1cm}b. Take the remaining groups as a set for training data\\

\hspace{1cm}c. Fit a model on the training set and evaluate it on the test set\\

\hspace{1cm}d. Retain the evaluation score and discard the model\\

5. Summarize the model using the sample of model evaluation scores\\

\end{mdframed}

%close to lecture notes
The advantage of cross-validation is that the data splitting is not done randomly so we don't get any unwanted influnece on the model building or prediction evaluation. This is called a  $k$-fold cross-validation structures the data splitting involves dividing the samples $k$ more or less equally sized exhaustive and
mutually exclusive subsets. In turn (at each split) one of these
subsets plays the role of the test set while the union of the
remaining subsets constitutes the training set. Such a splitting
warrants a balanced representation of each sample in both training and
test set over the splits. 
