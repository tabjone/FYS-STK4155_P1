%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}


%Motivation

%Why are statistical methods like these important, what are they used for
%What is OLS, ridge and lasso and why do we need it
%What is resampling and why do we need it

%Overarching ideas
%



%Structure of report
%We will start by looking at....
%Then ...
%This should be the same structure as theory, method and results are in
% So the structure is OLS, Ridge, Lasso on the Franke function. Then bootstrap and cross-validation on the franke-function
%Then the same on the real data
%%For bootstrap and cross-validation the point is to look at the effect lambda parameter and reduction in MSE, variance.



\begin{comment}
%motivation for resampling, from lecture notes
Resampling approaches can be computationally expensive, because they involve fitting the same statistical method multiple times using different subsets of the training data. However, due to recent advances in computing power, the computational requirements of resampling methods generally are not prohibitive. In this chapter, we discuss two of the most commonly used resampling methods, cross-validation and the bootstrap. Both methods are important tools in the practical application of many statistical learning procedures. For example, cross-validation can be used to estimate the test error associated with a given statistical learning method in order to evaluate its performance, or to select the appropriate level of flexibility. The process of evaluating a modelâ€™s performance is known as model assessment, whereas the process of selecting the proper level of flexibility for a model is known as model selection. The bootstrap is widely used.
\end{comment}