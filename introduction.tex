%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
In supervised machine learning, linear regression is an indispensable tool. It is used to find linear relationships between variables. We call the predicting variables independent at the variables being predicted for dependent. An example of where this is used is in blood pressure medication, where we can give a patient various dosages of certain drugs and watch how their blood pressure responds. Then we can find what drugs has the the highest correlation with lowering the blood pressure. 

We will look at three linear regression methods, called ordinary least squares (OLS), Least absolute shrinkage and selection operator (Lasso) and Ridge regression. The OLS method is fast and easy to implement but it can be inaccurate when we have highly correlated independent variables. The Ridge and Lasso method solves this by introducing a hyperparameter $\lambda$, which is used to regularize the model. Therefore depending on the data, all three models can be useful. The linear fitting will be done using the Vandermonde design matrix, which is a design matrix for a polynomial fit. Though there are many other ways to set up a design matrix.

%Write something about what dataset terrain data thing used. Norsk data greie eller noe
%This part here
The data used will be that of the Franke function, that is a function with two gaussian peaks and a dip, which is used for testing purposes. Then we will look at terrain data from the Oslo area. This data will be taken from the Space shuttle radar topography mission (SRTM) from year 2000 and will be downloaded from \href{https://earthexplorer.usgs.gov/}{https://earthexplorer.usgs.gov/}.

The data will be split into a training set, which we train the model on, and a test set, on which we will do the model fitting. Then we evaluate the model with the mean squared error, which we want to be as low as possible. And the score function, which we want to be as high as possible. We can then use these values to compare the different regression models.

%Also before this, mention overfitting? Here is a good place to mention beta parameter because we want plot beta as function of polynomial degree

Then we look at two resampling methods called the bootstrap and cross-validation. These are methods to draw repeated samples of the data and refit a model on each sample. This gives additional information that would not be attained by only fitting the model once. This will reduce the variance of the model, but likely also increase the bias which can make us miss relevant relations between the dependent and independent variables. This leads to what is called the Bias-Variance tradeoff, which is the problem of minimizing both the bias and the variance. Also resampling methods can be quite computationally expensive as we will have to refit the model over and over again.

This report is heavily based on the lecture notes by Morten Hjorth-Jensen, found on his github \href{https://github.com/CompPhysics/MachineLearning}{https://github.com/CompPhysics/MachineLearning} and the code for the project can be found at \href{https://github.com/jensjpedersen/Projects_FYS-STK4155/tree/main/Project1}{https://github.com/jensjpedersen/Projects_FYS-STK4155/tree/main/Project1}.