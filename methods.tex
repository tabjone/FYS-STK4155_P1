%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Method}
%%MOST IMPORTANT FOR METHOD (or results): Put in exact parameters used to produce plots so that a reader can reproduce it. %%

%%On how we implemented: pseudo-code and altos
%Maybe section about using SVD for inverting matrix, or maybe section on SVD in theory

%scaling of data. Why we did or didn't scale. Critical discussion of this.

\subsection{Datasets}
Our data was generated from a random uniform distribution in the interval [0,
1]. Two arrays was generated at random, with each containing n=20 data points. 
These arrays, x and y was combined in two mesh grids, X and Y each of size n x n. Thus, our
total dataset consisted of 400 data points. 

\subsubsection{Franke Function}
%What is it
%How did we implement it
%Why did we implement it
%Any testing to know its working correctly? (I think not needed)

The Franke Function was used to test and validate and test our models. 

The Franke function is a waited sum of four exponentials and takes two
arguments as input: 
\begin{align*}
    \label{eq:franke_function} 
    f(x,y) &= \frac{3}{4}exp\left(-\frac{(9x-2)^2}{4}-\frac{(9y-2)^2}{4} \right)
    + \frac{3}{4}exp\left(-\frac{(9x+1)^2}{49}-\frac{(9y+1)}{10} \right) \\
           &+ \frac{1}{2}exp\left(-\frac{(9x-7)^2}{4}-\frac{(9y-3)^2}{4}
           \right)-\frac{1}{5}exp(-(9x-4)^2-(9y-7)^2)
\end{align*}
This function is widely used for testing of different fitting algorithms. 
Before we applied our fitting algorithms on the terrain data, we used this function
to test and validate that our own developed algorithms worked properly.    

Random normal distributed noise with an amplitude of 0.2, zero mean and a
standard deviation of 1 was added to the data generated from the Franke
function. Our data was split in train and test data, with $20\%$ preserved for
testing purposes. Sklearn's trian\_test\_split function was used for splitting
of the data. In order to keep the dataset consistent between different runs, a
random seed generator was used. Our training data was fitted to polynomials up
to degree 12 in x and y. 



\subsubsection{Terrain data}
%How did we implement it
%Where did we find this
%Why do we use it

\subsection{Error estimation}
%%Mean squared error
%What is it
%Why do we use it

%%R^2
%What is it
%Why do we use it

\subsection{Linear regression}

%We used the Vandermonde design matrix for a linear regression polynomial fit. No point in having a theory section about this.
\begin{center}
$V=
\begin{bmatrix} 
1 & x_{0}&x_{0}^{2}&\dots &x_{0}^{p-1}
\\1&x_{1}&x_{1}^{2}&\dots &x_{1}^{p-1}
\\1&x_{2}&x_{2}^{2}&\dots &x_{2}^{p-1}
\\ \vdots &\vdots &\vdots &\ddots &\vdots \\
1&x_{n-1}&ax_{n-1}^{2}&\dots &x_{n-1}^{p-1}
\end{bmatrix}
$
\end{center}


%%OLS
%How did we implement it
%Why did we implement it
%Did we do any testing

%%Ridge
%How did we implement it
%Why did we implement it
%Did we do any testing

%%Lasso
%How did we implement it
%Why did we implement it
%Did we do any testing

\subsection{Resampling}
%%Bootstrap
%How did we implement it
%Why did we implement it
%Did we do any testing

%%Cross-validation
%How did we implement it
%Why did we implement it
%Did we do any testing




%THIS IF FROM THE LECTURE NOTES. BUT ABIT EXTENSIVE. MIGHT NOT NEED IT
\begin{comment}
How to set up the cross-validation for Ridge and/or Lasso

* Define a range of interest for the penalty parameter.

* Divide the data set into training and test set comprising samples $\{1, \ldots, n\} \setminus i$ and $\{ i \}$, respectively.

* Fit the linear regression model by means of ridge estimation  for each $\lambda$ in the grid using the training set, and the corresponding estimate of the error variance $\boldsymbol{\sigma}_{-i}^2(\lambda)$, as

\begin{align*}
\boldsymbol{\beta}_{-i}(\lambda) & =  ( \boldsymbol{X}_{-i, \ast}^{T}
\boldsymbol{X}_{-i, \ast} + \lambda \boldsymbol{I}_{pp})^{-1}
\boldsymbol{X}_{-i, \ast}^{T} \boldsymbol{y}_{-i}
\end{align*}

* Evaluate the prediction performance of these models on the test set by $[y_i, \boldsymbol{X}_{i, \ast}; \boldsymbol{\beta}_{-i}(\lambda), \boldsymbol{\sigma}_{-i}^2(\lambda)]$. Or, by the prediction error $|y_i - \boldsymbol{X}_{i, \ast} \boldsymbol{\beta}_{-i}(\lambda)|$, the relative error, the error squared or the R2 score function.

* Repeat the first three steps  such that each sample plays the role of the test set once.

* Average the prediction performances of the test sets at each grid point of the penalty bias/parameter. It is an estimate of the prediction performance of the model corresponding to this value of the penalty parameter on novel data. It is defined as

\begin{align*}
\frac{1}{n} \sum_{i = 1}^n \log\{L[y_i, \mathbf{X}_{i, \ast}; \boldsymbol{\beta}_{-i}(\lambda), \boldsymbol{\sigma}_{-i}^2(\lambda)]\}.
\end{align*}
\end{comment}

