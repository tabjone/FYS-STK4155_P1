{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAY IN THEORY THAT: design matrix can be model with x,y so we get cross terms ect in matrix, like in Franke.\n",
    "\n",
    "Expectation value, MSE and Variance (error noe eller evaluation)\n",
    "\n",
    "Notes: predictors er $p$ i pxn design matrix. Altsa linear factors i polynomials\n",
    "\n",
    "\n",
    "Vandermonde matrix: Polynomial design matrix.\n",
    "\n",
    "Linear regression models are often fitted using the least squares approach, but they may also be fitted in other ways, such as by minimizing the \"lack of fit\" in some other norm (as with least absolute deviations regression), or by minimizing a penalized version of the least squares cost function as in ridge regression (L2-norm penalty) and lasso (L1-norm penalty). Conversely, the least squares approach can be used to fit models that are not linear models. Thus, although the terms \"least squares\" and \"linear model\" are closely linked, they are not synonymous.\n",
    "\n",
    "\n",
    "A cost function is an important parameter that determines how well a machine learning model performs for a given dataset. It calculates the difference between the expected value and predicted value and represents it as a single real number.\n",
    "\n",
    "Remember that hat beta = +- delta hat beta, delta hat beta = sqrt var beta\n",
    "\n",
    "State answers of E and var of y_i, E and var of beta in teori and ref to appendix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression\n",
    "In machine learning we call the independent variable $\\mathbf{x}$ a feature and the dependent variable $\\mathbf{y}$ a response. A regression model aims at finding a likelihood function $p(\\mathbf{y}|\\mathbf{x})$, that is the conditional distribution for $\\mathbf{y}$ with a given $\\mathbf{x}$. The estimation of $p(\\mathbf{y}|\\mathbf{x})$ is made using a data set with $n$ cases $i=0,1,2,...,n-1$ of a response variable $y_i$ and a set of predictor variables $\\mathbf{x}_i=[x_{i0}, x_{i1},...,x_{ip-1}]$. The set of these vectors $\\mathbf{X}=[\\mathbf{x}_{0}\\ \\mathbf{x}_{1}\\ ...\\ \\mathbf{x}_{n-1}]$ is called the design matrix of the model. \n",
    "\n",
    "\n",
    "The aim of regression analysis is to explain $\\mathbf y$ in terms of $\\mathbf X$ through a functional relationship like $y_i=f(\\mathbf{X}_{i,∗})$. When no prior knowledge on the form of $f(⋅)$ is available, it is common to assume a linear relationship between $\\mathbf{X}$ and $\\mathbf{y}$. This assumption gives rise to the linear regression model where $\\boldsymbol\\beta=\\left[\\beta_0, \\beta_1, ..., \\beta_{p-1} \\right]^T$ are the regression parameters and the error variable $\\boldsymbol\\epsilon$ is an unobserved random variable that adds \"noise\" to the linear relationship between the dependent variable and regressors, so that our model becomes\n",
    "\n",
    "$$\n",
    "\\tilde{y}_i(x_i) = \\sum_{j=0}^{p-1} \\beta_j x_{ij}.\n",
    "$$\n",
    "\n",
    "Giving that the dependent variable is\n",
    "\n",
    "$$\n",
    "y(x_i) = \\tilde{y}_i + \\varepsilon_i = \\sum_{j=0}^{p-1} \\beta_j x_{ij} + \\epsilon_i.\n",
    "$$\n",
    "\n",
    "These $n$ equations can be written on matrix form as\n",
    "\\begin{equation}\\label{eq:linear_regression}\n",
    "    \\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}.\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Ordinary least squares\n",
    "The method of ordinary least squares computes the unique line that minimizes the sum of squared differences between the true data and that line. \n",
    "\n",
    "We define the cost function for ordinary least squares as the mean squared error\n",
    "\n",
    "$$\n",
    "C(\\boldsymbol\\beta) = \\frac{1}{n}\\sum_{i=0}^{n-1}(y_i-\\tilde{y}_i)^2=\\frac{1}{n}\\left[(\\mathbf{y}-\\mathbf{\\tilde y})^T(\\mathbf{y}-\\mathbf{\\tilde y}) \\right].\n",
    "$$\n",
    "\n",
    "Inserting the predicted values $\\mathbf{\\tilde y}=\\mathbf{X}\\boldsymbol\\beta$ we get\n",
    "\n",
    "$$\n",
    "C(\\boldsymbol\\beta) = \\frac{1}{n}\\sum_{i=0}^{n-1}(y_i-\\tilde{y}_i)^2=\\frac{1}{n}\\left[(\\mathbf{y}-\\mathbf{X}\\boldsymbol\\beta)^T(\\mathbf{y}-\\mathbf{X}\\boldsymbol\\beta) \\right].\n",
    "$$\n",
    "\n",
    "We call optimal parameter $\\hat{\\boldsymbol\\beta}$ the one that minimizes the cost function. This will be a zero of the derivative\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\boldsymbol\\beta}{\\partial \\beta_j} = 0.\n",
    "$$\n",
    "\n",
    "Doing this derivative we get that\n",
    "\n",
    "$$\n",
    "\\mathbf{X}^T (\\mathbf{y}-\\mathbf{X}\\hat{\\boldsymbol{\\beta}} )=0.\n",
    "$$\n",
    "\n",
    "Rewriting this we get that\n",
    "\n",
    "\\begin{equation}\\label{eq:beta_OLS}\n",
    "    \\hat{\\boldsymbol\\beta}=(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\n",
    "\\end{equation}\n",
    "is the optimal parameter if the Hessian matrix $\\mathbf{H}=(\\mathbf{X}^T\\mathbf{X})$ is invertible.\n",
    "\n",
    "\n",
    "### Ridge\n",
    "\n",
    "### Lasso\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Franke function\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In appendix, mean and var of y_i, and of beta\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
