%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results and Discussion}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Franke Data}
\begin{figure}[H]
    \centering \includegraphics[width=0.8\textwidth]{Figures/franke_data_and_model_ridge_best.png}
    \caption{Left figures shows the frank function plotted as a function of x
    and in y coordinates the intervall [0,1]. The red and yellow dots shows or
train- and test-data with added normal distributed noise, $0.2 \cdot
\sigma(0,1)$. The lowest MSE, calculated with bootstrap re-sampling was obtained
with Ridge regression with a regularization parameter $\lambda = 10^{-5}$ and a
polynomial degree of 5. Right figure shows the best fit model. Train- and
test-data is plotted as for the left figure.} 
\end{figure}

\begin{figure}[H]
     \centering
     \begin{subfigure}[t]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figures/franke_ols_ridge_lasso_boots.png}
         \caption{Mean MSE score for 100 bootstrap re-samples. MSE scores is
             plotted as a function of polynomial degree, for the best fit
             models obtained with OLS, Ridge ($\lambda =
             10^{-j}$) and Lasso ($\lambda = 10^{-5}$) regression.}  
         \label{} 
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figures/franke_ols_ridge_lasso_kfold.png}
         \caption{Mean of MSE scores calculated on all ten test folds with K-fold
             re-sampling. Our best fit models from OLS, Ridge ($\lambda =
             10^{5}$) and Lasso ($10^{-5}$) regression
     is plotted as a function of polynomial degree. $\lambda $ is the 
 regularization parameter.}  
         \label{fig}
     \end{subfigure}

\end{figure}


\begin{table}
    \centering
    \caption{Table of Best mean squared error (MSE) scores obtained with
        different re-sampling and regression
        methods. The training- and testing-data splits from the Franke function
        was used for model fitting and
        prediction respectively. $\lambda $ is the best choice of regularization parameters for
        Ridge and Lasso regression. n refers to number of re-samples/splits used in
        Bootstrap and K-fold.}  
    \label{tab:franke_mse_best} 
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        Polynomial degree & $\lambda$ & Regression method & Re-sampling method & MSE \\
        \hline
                      4    &    & OLS & Bootstrap (n=100) & 0.04440 \\
        \hline
                      5   & $10^{-4}$  & Ridge & Bootstrap (n=100)& 0.04425 \\
        \hline
                      11  & $10^{-5}$  & Lasso & Bootstrap (n=100)& 0.04673 \\
                      
        \hline
                      5    &   & OLS &  K-fold (n=10) &  0.04181\\
        \hline
                      6   &  $10^{-5}$  & Ridge &  K-fold (n=10) & 0.04155 \\
        \hline
                      12  &  $10^{-5}$ & Lasso &  K-fold (n=10) & 0.04548 \\
        \hline
    \end{tabular} 
\end{table}

% TODO OLS scores

%%%%%%%%%%%%%%% Parameters %%%%%%%%%%%%%%%
% n_data = 20 - n_data in x and y: n_tot = 20*20
% test_size = 0.2
% noise = 0.2 - aptitude of normal distributed noise    
% data_dim = 2

%%%%%%%%%%%%%%% Part b %%%%%%%%%%%%%%%
% * Evaluate MSE up to 5.th order fro OLS. 
% * and R2 score
% * blot parameters beta
% * Your code has to include a scaling/centering:
%   XXX: not included, data is already scaled in intervall [0, 1]

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.5\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figures/b_mse.png}
     \end{subfigure}%
     \hfill
     \begin{subfigure}[b]{0.5\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figures/b_r2.png}
     \end{subfigure}
        \caption{Mean squared error and score function for the Ordinary Least
        Squares (OLS) method on the Franke function for training and test data
    with a test/train split of 0.2}
        \label{fig:mse_and_score_franke}
\end{figure}

% Results and discussion of r2 and mse franke plots.  
In figure \ref{fig:mse_and_score_franke} we see the mean squared error and R2
score on the Franke function for a polynomial up to degree five. The Predicted
MSE scores with respect to polynomial degree was predicted on both the training
aand test data. As expected out model gets better as the polynomial degree
increases. That is, our MSE score decreases and R2 score increases. For
polynomial degree 5, the MES score increase for the test data and decreases for
the training data. The opposite behavior is seen for the R2 score. This is an
expected behavior. For increases polynomial degree we should expect that
predictions on the training data gets better and betters, since our model is
trained on the exact same data. Thus, for the prediction on training data we
expect the MES to approach zero as the polynomial degree increases. The same
behaviour can not be expected when we predict MES and R2 score on the test
data. Our model has never seen the test data. We expect that the MES score
predicted on the test data decreases to a certain polynomial degree. Afterward
it will increase, when we have reached over fitting. That is, our model is
fitted to well to the training data, and is no longer generalizable to other
similar distributed datasets. This is the exact behavior we observe in figure
\ref{fig:mse_and_score_franke}. However, the effect if over fit is not
presented very well cause of the low polynomial degree. 


%\begin{figure}[H]
%    \centering
%    \caption{Mean square error as function of polynomial degree for }
%    \label{fig:ols_franke}
%    \includegraphics[width=0.8\textwidth]{Figures/b_mse.png}
%\end{figure}

%\begin{figure}[H]
%    \centering
%    \caption{}
%    \label{fig:score_franke}
%    \includegraphics[width=0.8\textwidth]{Figures/b_r2.png}
%\end{figure}

% TODO: error bars
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/b_beta.png}
    \caption{ $\beta _i$ values plotted with respect polynomial degree for our best
    fit model with OLS on the training data, $\bm{X} _{train} $ 
    The colors represent specific $\beta _i$, in order to make it possible to
    track specific $\beta _i$ for different polynomial degrees.      }
    \label{fig:beta_plot}
\end{figure}

% Discussion of beta's
In figure \ref{fig:beta_plot} we see for each polynomial degree the beta
parameters $\beta_i$ as a dot in the plot. This is for the OLS model on the
training data from the Franke function. We see that as the polynomial degree increases, the $\beta$
values deviate more and more from zero. This can be a result of over fitting
since we do not have any regularization parameter on the OLS model.

We expect the different $\beta _i$ does not vary significantly with increasing
polynomial degree. It's not easy to say if this is the case from inspection of
the plot, due to a limited number of colors and large number of $\beta _i$. It seems to be the case for the
lower $\beta _i$ terms. It does not make intuitively sense if a certain $\beta _i$ varies a
lot for different polynomial degree's. The less complex model (lower poly deg) should to a certain degree
reflect the values in a more complex model. By introducing higher polynomials
and hence more $\beta _i$'s, our model can represent more and more complex
patterns (more variations in shorter length scale). However we expect the lower
oerder $\beta _i$ to be more or less the same. Because they represent the less
complex parts of our model. 
 
% TODO: Discuss variance of beta


%For bootstrap: should consider plotting a histogram of the estimators beta_hat^* as this should resemble a pdf. 

%%%%%%%%%%%%%%% Part c  %%%%%%%%%%%%%%%
% * Explain bias, variance, mse terms (theory) and interpretation - XXX: where
% is this done?
% * Bias variance analysis on franke function
% * discuss in bias variance trade-off in terms of:
%   * model complexity
%   * number of data points
%   * training and test data

%%bootstrap figures for different number of datapoints
\begin{figure}
     \centering
     \begin{subfigure}[b]{0.5\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figures/c_bootstrap_ols_n_data_400.png}
         \caption{400 datapoints}
     \end{subfigure}%
     \hfill
     \begin{subfigure}[b]{0.5\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figures/c_bootstrap_ols_n_data_1600.png}
         \caption{1600 datapoints}
     \end{subfigure}
        \caption{Bootstrap with 100 resamples for the OLS method on the Franke function on the test set.}
        \label{fig:bootstrap_ols_franke_test}
\end{figure}


In figure \ref{fig:bootstrap_ols_franke_test} we can see the bootstrap used
on the OLS method on the Franke function for 100 resamples and two different
number of datapoints. We see that for the lower number of points we get signs
of overfitting, while we do not see this behaviour at these polynomial degrees
for 1600 datapoints. We see that for 400 datapoints the optimal bias-variance
tradeoff happens at a polynomial degree of around 4 and for 1600 datapoints at
around polynomial degree 7. 

This means that our best fit model is depend the size of the dataset. For
larger dataset sizes the mean value of our data points will resemble more and
more the real distribution. This is also the case for our training data and
test data. To achieve over fit the complexity of the model needs to be very
large.  

In figure \ref{fig:bootstrap_ols_franke_train} the MSE predicted on the training
data with OLS and bootstrap resampling is plotted as function of polynomial
degree. Here we see a similar trend as for the predictions with bootstrap on
the test data. If we had trained the model on the real training data, we would
expect that the predictions of MSE on the training data approaches zero. Our
predicted model should be able to 100\% replicate our data, since the model is
trained the exact same noise. In this scenario it does not make much sense to
talk about over fitting. Cause we need two independent datasets to predict the
generalizability of our model. 

However in our case we did re sample the training dataset (bootstrap with 100
resamples) before we trained our
model. Thus, our re sampled dataset should follow the same distribution, but each
individual data points would vary from or original training data. As long as
the number of data points in our original training data is sufficiently large.
That is, the expected value of our noisy model for the Franke function
approaches the Franke funciton without noise. We expect that prediction on
re sampled data VS. a separate test split would yield similar results. By
comparison of figure \ref{fig:bootstrap_ols_franke_test} (left) and
\ref{fig:bootstrap_ols_franke_train}, this is what we observe. 

% XXX: Is this expected behaviour?
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/c_bootstrap_ols_n_data_400_train_data.png}
    \caption{Bootstrap with 100 resamples for the OLS method on the Franke
    function on the training set with 400 datapoints.}
    \label{fig:bootstrap_ols_franke_train}
\end{figure}

Until now we have discussed our models in terms of MSE. However MSE is clolsy
related to bias and variance (see eq. \eqref{eq:bias_variance_decomposition}).
The variance describes the precision on our predicted models. As our model
complexity increases, it will be able to pick up complex patterns in our
training data. If we use another training dataset this pattern may not be
contained in the data. Thus our complex model fit will give different results.
If our models gets to complex it will describe patters that are unique to the
training data and not the representative distribution. This is called
over fitting and can be observed when the variance term rapidly increases with
model complexity. We previous saw that the MSE rapidly increases after a
certain polynomial. The reason is that the variance of the model increses, as
seen in figure \ref{fig:bootstrap_ols_franke_test} b and
\ref{fig:bootstrap_ols_franke_train}. 

The bias term is expected to go to zero as the model complexity increases. That
is our expected value of our models is approaching the real function. In our
results the bias term is decreasing (see fig \ref{fig:bootstrap_ols_franke_test} b and
\ref{fig:bootstrap_ols_franke_train}), as expected. But after a certain
polynomial degree the bias term is slightly increseing. This is somewhat
unexpected. However this may be due to the fact that the variance is
increseing and it's harder to obtain a good statistic on the real expectation
value of our model. This may have been resolved by increasing the number of
bootstrap re samples. It's also worth metntioning that we did not used the
exact function f(x) to esitamte the bias term. We used out test data, $\bm{y}
_{test} $. That is our exact function (Franke function) plus added normal
distributed noise. 
% This is probably why the bias term does not behave as
% expected. The statistics on our model (Expectation value) 
% TODO: think more





% To check the speculation of overfitting on the lower number of datapoints we
% look at figure \ref{fig:bootstrap_ols_franke_train} and we see here that the
% variance shoots off as the polynomial degree gets to 12. 

%%%%%%%%%%%%%%% Part d %%%%%%%%%%%%%%%
% Use k-fold and evaluate MEE on test data
% compare with bootstrap
% compare with sklearn


\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/d_kfold_ols_n_10.png}
    \caption{Mean squared error using cross-Validation with 10 splits on the Franke function.}
    \label{fig:kfold_ols_franke}
\end{figure}

In figure \ref{fig:kfold_ols_franke} we can see the MSE of Cross-validation
with 10 splits on the Franke function using OLS. This is on the test data. We
see that polynomial degree over 8 produces weird results. This can be because
of over fitting. 
K-fold cross validation produces similar results as for bootstrap up to degree
8. Then the MES predicted for k-fold rapidly increases for polynomial degree 9.
For bootstrap this increases is seen for polynomial 12. We used bootstrap to
infer statistics on our data. And we re sampled the training data. Thus
statistic inferred from our bootstrap re sampling is representative to how our
training data is distributed. If our test and training data has different
distributions, or the number of data points is to small to estimate the real
expectation value of our data. Then, K-fold and Bootstrap is expected to
disagree on statistics. K-fold divides the dataset in splits and tests/trains
on all the splits. In this way we can be sure that our scores is not biased
towards our particular train/test split. Our results from bootstrap and K-fold
largely argreess with each other. This means that our original chosen
train/test split is representative four our dataset. Thus, if we had done a new
train test split, it's expected that the results would have been similar.
Hence, the precision of our generalized model is good. 

%%%%%%%%%%%%%%% Part e %%%%%%%%%%%%%%%
% * bootstrap analysis for ridge as in part c
% * and cross validation as in part d 
% * Compare results to those obtained in part b-d
% * study bias variance trade off for different values of lambda 

The minimum MSE from Ridge regression with bootstrap re sampling was found for polynomial degree of 6 with
the hyper parameter, $\lambda = 0.001$. This is seen in figure
\ref{fig:ridge_blabla}. There is no sign of over fitting in this figure. Both
the variance, bias and hence MSE are approximate constant for polynomial 3-12.
This is because we have introduced a regularization parameter that's sets an
upper bound on the second norm of the betas. For OLS when we increase the
polynomial degree, the number of linearly depended columns in our design matrix
increases. This causes high variance in betas's (see eq.
\ref{eq:variance_beta}), and hence a high variance in our model.  
This is not seen in case of Ridge regression.      
% TODO: how ridge shrinks betas

In Figure \ref{fig:e_ridge} the predicted MSE with bootstrap re sampling on the test data as function of
polynomial degree and different $\lambda $ is plotted in a heat map. We observe
that the variation in MSE decreases with increasing $\lambda $. Thus, high
$\lambda 's$ tends to shrink the $\beta's $ to zero.  

In Figure \ref{fig:e_ridge_kfold} the same heat map as for \ref{fig:e_ridge} is
plotted, but instead with K-fold re sampling. Again we observe similar values
as for bootstrap. Thus, the statistics inferred with bootstrap re sampling is
generalizable to the whole dataset. 
% TODO: write more



% Here we are to low in polynomial degree to see the
% typical bias-variance pattern.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/e_ridge_bias_variance_lamb_0_001.png}
    \caption{Ridge resgression on the Franke function.}
    \label{fig:ridge_blabla}
\end{figure}


\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Figures/e_ridge_n_boots_100.png}
    \caption{Correlation between MSE, the hyperparameter $\lambda$ and polyonmial degree for bootstrap on Ridge for on the Franke function.}  
    \label{fig:e_ridge} 
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Figures/e_ridge_kfold_n_10.png}
    \caption{Correlation between MSE, the hyperparameter $\lambda$ and polyonmial degree for Cross-Valdidation for Ridge regression on the Franke function.}
    \label{fig:e_ridge_kfold}
\end{figure}


%%%%%%%%%%%%%%% Part f %%%%%%%%%%%%%%%
% * Lasso regression
% * give a critical discussion of mse, ridge, lasso, in theory I discussed benefits of methods.
% * Which model fits the data best 
% * bootstrap bias variance analysis of lasso
% * MSE analysis with kfold


In Figure \ref{fig:f_lasso_bootstrap} the MSE predicted on the test data with
Lasso regression and Bootstrap re sampling is plotted as function of polynomial
degree with respect different choices of the regularization parameter $\lambda
$. Form this heatmap we can observe that decreasing with increasing polynomial
degree for lower order polynomials and small values for $\lambda $. That is,
the bias of our model is decreasing. However for higher values of $\lambda $
the MSE score is more or less constant with respect to polynomial degree.
Regularization parameters larger than 0.01 zeros out the beta values for our
model. We observer that the best values for MSE is obtained for polynomial
degree 12. Lasso regression gives similar results as for Ridge regression, with
slightly higher MSE scores. The best fit model model with ridge regression is
observed for lower polynomial degree's (range 4-8). 

In Figure \ref{fig:f_lasso_kflod} the MSE predicted on the test data with Lasso
regression and K-fold cross validation is plotted as function of polynomial
degree with different values of $\lambda $. Again the magnitude of MSE scores
largely agrees with those obtained with Bootstrap. Generally MSE scores is
slightly larger than for Bootstrap. This is however not case for polynomial
degree 11-12 where the K-fold methods gives us lower MSE scores compared
Bootstrap.  

In table \ref{tab:franke_mse_best} our best fit models for the different
re sampling and regression methods are summarized. As observed from the heat
maps, Lasso regression tend to decrease with increasing polynomial degree and
gives us the lowest MSE score for polynomial degree 11 and 12 with Bootstrap and
K-fold cross validation respectively. Ride and OLS regression gives us the best
fit model in the polynomial degree range of 4-6. Lasso regression performs the
worst it terms of MSE. OLS and Ridge gives similar MSE scores for their best
fit models. However Ridge slightly outperforms OLS, for K-fold. However the
opposite is true for Bootstrap re sampling. The disadvantage with OLS compared
to Ridge is that it is very prone to over fitting.  

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Figures/f_lasso_bootstrap_n_100.png}
    \caption{Correlation between MSE, the hyperparameter $\lambda$ and polyonmial degree for Cross-Valdidation for Lasso regression on the Franke function.}
    \label{fig:f_lasso_bootstrap}
\end{figure}


\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Figures/f_lasso_kfold_n_10.png}
    \caption{Correlation between MSE, the hyperparameter $\lambda$ and polyonmial degree for Cross-Valdidation for Lasso regression on the Franke function.}
    \label{fig:f_lasso_kflod}
\end{figure}



% TODO: comparison of MSE for all methods
 
 %Maybe split results into two parts, Franke and Terrain data

%According to red line in report we want:
%On Franke-Function
%Subplot of MSE and R^2, test and training data, for OLS
%Subplot of MSE and R^2, test and training data, for Lasso
%Subplot of MSE and R^2, test and training data, for Ridge
%Comparrison of Regression methods
%Beta plot, but how to do this nicely, to show how beta values get large when polynomial degree increases.
%Resampling: bias-variance plot of bootstrap and cross-validation
%Study lambda dependence (correlation plot)

%On Terrain-data
%Subplot of MSE and R^2, test and training data, for OLS
%Subplot of MSE and R^2, test and training data, for Lasso
%Subplot of MSE and R^2, test and training data, for Ridge
%Comparrison of Regression methods
%Resampling: bias-variance plot of bootstrap and cross-validation
%Study lambda dependence (correlation plot)

%For critical discussion of centering/scaling, from lecture notes: "If our predictors represent different scales, then it is important to standardize the design matrix  by subtracting the mean of each column from the corresponding column and dividing the column with its standard deviation."




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Terrain data}

The best MSE values obtained with different regression and re sampling methods
is listed in table \ref{tab:terrain_mse_best}. Figure \ref{fig:terrain_model}
shows our dataset (left figures) compared to our best fit model with OLS (right
figure). 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/terrain_data_and_model_ols_best.png}
    \caption{The left figure shows the surface of our sliced terrain data. Our
    train and test-data are represented by yellow and purple dots respectively.
Hight above sea level in meters is plotted on the z axis. The x- and y-axis is
takes values in intervall [0, 1]. This interval scales to 30m in real
coordinates. The left figure shows our best fit model obtained with OLS for
polynomial degree 18. Again the training- and test-data is plotted as in the
left figure.}  
    \label{fig:terrain_model} 
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Figures/terrain_best_ols_ridge_lasso_boots.png}
        \caption{Mean of MSE scores calculated with bootstrap on the terrain data.\\ $n_{\text{boots}} =
        20$ re-samples was used. The mean MSE predicted on the test-data is plotted as
    function of polynomial degree for OLS-, Ridge- ($\lambda = 10^{6}$) and
Lasso-regression ($\lambda = 10^{-6})$   }  
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Figures/terrain_best_ols_ridge_lasso_kfold.png}
        \caption{Mean of MSE predicted on each test fold with use of the K-fold
        re-sampling technique. A total of ten folds was used. The MSE score is plotted as a function
    of polynomial degree for OLS-, Ridge- ($\lambda = 0.1$) and Lasso-regression
($\lambda = 10^{-4}$)}  
    \end{subfigure}

    \caption{Comparison of OLS, Ridge and Lasso regression}  
    \label{fig:terrain_ols_vs_ridge_vs_lasso} 
\end{figure}

% Comparison of OLS, Ridge and Lasso
In Figure \ref{fig:terrain_ols_vs_ridge_vs_lasso} the MSE on the terrain test
data is predicted with for the best fit regression methods with Bootstrap and
K-fold cross validation. The MSE socre obtined with OLS and bootstrap reaches a minima for
polynomial degree 18. This is not the case for K-fold cross validation, where
the MSE explodes for polynomial degree 5 and higher. The bootstrap re sampling
was done on the selected training data. If we had chosen a new training data split,
we would expect a different result for our MSE score. This is what we show with
the K-fold technique. The terrain is to complex, with to few data points to
create a generalizable model of the terrain with OLS regression. For our
optimal values of regularization parameters with ridge and lasso regression, we
observe a trend of decreasing MSE scores with increasing polynomial degree. For
ridge regression  and lasso regression we got a minima for polynomial degree 18
and 30 respectively. Both gives scores that are higher than with OLS. But we
dont see the increase in variance as for OLS regression. The MSE score for
Lasso and Ridge is almost constant from polynomial degree 14-30. The reason is
that, our
penalty term kills some of the higer order beta terms (highly correlated
features) that causes the unstable % XXX double check that this is correct
behaviour seen with OLS. We observe that Lasso produces a higer MSE vale than
with
Ridge regression. This is probably due to the fact that Lasso zeros out the highly correlated
features compared to Ridge that shrinks the beta's. 

The best regularization parameters with Ridge regression and bootstrap was found from
inspection of Figure \ref{fig:g_ridge_boost_heatmap}. Here all the MSE scores
predicted on our training data is plotted from polynomial degree 10-30 and
regularization parameters. We observer that all MSE scores tend to increase for
higher values of $\lambda $. That is, the complexity of our model is reduced
for higher values of $\lambda $

In figure \ref{fig:g_ridge_kfold_heatmap} the same heat map is plotted, but
with K-fold cross validation instead of bootstrap. Here we observe almost the
opposite trend than for bootstrap re sampling. The MSE score tends to explode for
small values of $\lambda $ and with increasing polynomial degree. It 
indicates that complexity of data is not well represented by the higer order
$\beta $'s. Thus, by intruding more dominant penalty term the complexity of our
model is reduced and we don't get a problem with over fitting. 

In Figure \label{fig:g_lasso_boots_heatmap} the heat map for Lasso regression
with bootstrap re sampling is plotted. The heatmap is plotted from polynomial
degree 10-30. However our best MSE score was found at polynomial degree 7 with
$\lambda = 0.001$. The reason why this value is not shown in the heat map is to
keep it consistent with the other heat maps for comparison reasons. Lasso
regression yield higher values for the MSE than ridge regression. Generally the
best scores for the different $\lambda $ is seen for polynomial 30. Also the
standard deviation with respect to Polynomial degree is generally lower than
for ridge. That is the penalty the correlated highly correlated features is
higher than for ridge.  

In figure \ref{fig:g_lasso_boots_heatmap} the heat map with Lasso regression
and K-fold cross validation is plotted. Again, as for ridge regression we
observe over fitting for the lower values of $\lambda $.
The MSE scores increases from polynomial degree 17-30 for $\lambda = 10^{-6}$
and $\lambda  = 10^{-5}$. For $\lambda = 0.1$ and $\lambda = 1.0$ the MES score
is almost constant as function of polynomial degree. This is because the
$\beta's$ is zeroed out. Lasso gives is less forgiving than Ridge which shrinks
the parameters for $\beta$. 

As explained in section \ref{sec:theory} the MSE can be decomposed in a bias,
variance and irreducible error term. This bias variance decompositions is shown
in Figure \ref{fig:g_ols_bias_variance_boots} for OLS regression with
bootstrap, predicted on our test data. Again as shown for the Franke
data, we observe that Large increase on our MSE score for polynomial 30 is
explained by the variance term. In figure
\ref{fig:g_ols_ridge_lasso_boots_n_20} we observer the R2 score for the same
model. The R2 score is highly related to the MSE and we observe the opposite
trend. We chose not to include this bias variance plot for Ridge and Lasso
regression. Because this does not provide us any more information than the MSE.
We know that the reduction in MSE for low polynomial is due to the decrease in
bias. And the increase for higer order polynomials is due to the increase in
variance. 


\begin{figure}
\centering
\begin{subfigure}{0.49\textwidth}
    \includegraphics[width=\textwidth]{Figures/g_ols_bias_variance_boots_n_20.png}
    \caption{Bias, Variancse and MSE obtained with OLS and bootstrap re-sampling.
    20 re-samples was used to calculate the mean scores.}
    \label{fig:g_ols_bias_variance_boots}
\end{subfigure}
\hfill
\begin{subfigure}{0.49\textwidth}
    \includegraphics[width=\textwidth]{Figures/g_ols_r2_boots_n_20.png}
    \caption{R2 score obtained with OLS and bootstrap re-sampling.
    20 re-samples was used to calculate the mean R2 score.}
    \label{fig:g_ols_bias_variance_boots}
\end{subfigure}
\label{fig:g_ols_sub_plot}
\end{figure}


\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Figures/g_ridge_heatmap_boots_n_20.png}
    \caption{Heatmap of MSE scores obtained with Ridge regression and Bootstrap
    re-sampling, with 20 bootstrap re-sampling iterations.}  
    \label{fig:g_ridge_boost_heatmap}  
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Figures/g_ridge_heatmap_kfold_n_10.png}
    \caption{Heatmap of MSE scores obtained with Ridge regression and K-fold
    re-sampling, with 10 splits.}  
    \label{fig:g_ridge_kfold_heatmap}  
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Figures/g_lasso_heatmap_boost_n_20.png}
    \caption{Heatmap of MSE scores obtained with lasso regression and Bootstrap
    re-sampling, with 20 re-samples.}  
    \label{fig:g_lasso_boots_heatmap}  
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Figures/g_lasso_heatmap_kfold_n_10.png}
    \caption{Heatmap of MSE scores obtained with lasso regression and K-fold
    re-sampling, with 10 splits.}  
    \label{fig:g_lasso_boots_heatmap}  
\end{figure}

In figure \ref{fig:g_ridge_boost_heatmap}, \ref{fig:g_ridge_kfold_heatmap}, \ref{fig:g_lasso_boots_heatmap} and \ref{fig:g_lasso_boots_heatmap} we see the MSE for a number of polynomial degreees and $\lambda$'s on the terrain data function. We notice that for Lasso a bigger $\lambda$ tend to provide smaller MSE, while for Ridge it's the opposite. This means the OLS method and the Ridge method would produce similar results. 

\begin{table}
    \centering
    \caption{Table of Best mean squared error (MSE) scores obtained with
        different re-sampling and regression
        methods. $\lambda $ is the best choice of regularization parameters for
        Ridge and Lasso regression. n refers to number of re-samples/splits used in
        Bootstrap and K-fold.}  
    \label{tab:terrain_mse_best} 
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        Polynomial degree & $\lambda$ & Regression method & Re-sampling method & MSE \\
        \hline
                          18 &   &  OLS & Bootstrap (n=20) & 3.12\\
        \hline

                          24 & $10^{-6}$ &   Ridge & Bootstrap (n=20) & 4.22\\
        \hline

                          30 &  $10^{-6}$ & Lasso&  Bootstrap (n=20) & 8.88\\
        \hline

                          5 &  & OLS & K-fold (n=10)& 21.23\\
        \hline

                          8 &  0.1 & Ridge & K-fold (n=10)& 18.74\\ % XXX: new - forgot to inspect deg 0-10
                          % 19 &  0.01 & Ridge & K-fold (n=10)& 19.6  XXX: old
        \hline
                          7 & 0.001 &  Lasso & K-fold (n=10)& 18.77\\ % XXX: new
                          % 19 & 0.01 &  Lasso & K-fold (n=10)& 19.73\\ % XXX: old
        \hline
    \end{tabular} 
\end{table}

% We see in table \ref{tab:terrain_mse_best} that the optimal polynomial degrees are higher for Ridge and Lasso both with bootstrap and Cross-Validation.



% \begin{figure}[H]
%     \centering
% 	\label{fig:g_ols_ridge_lasso_boots}
% 	\caption{Mean squared error as a function of polynomial degree for the regression methods OLS, Ridge and Lasso using ? number of bootstraps on the terrain data.}
%     \includegraphics[width=0.8\textwidth]{Figures/g_ols_ridge_lasso_boots_n_20.png}
% \end{figure}

% In figure \ref{fig:g_ols_ridge_lasso_boots} we can see the MSE for the three
% different regression methods. We see that Lasso and Ridge regression is more
% stable but OLS beats both methods at around polynomial degree 12. But then we
% see overfitting of the OLS method at higher degree polynomials.



